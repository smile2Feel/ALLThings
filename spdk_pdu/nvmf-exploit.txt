nvme常用命令：

客户端：
1. 加载驱动 modprobe nvme_fabrics  Failed to write to /dev/nvme-fabrics: Invalid argument
2. discover：nvme discover -t rdma -a 172.87.29.65 -s 4420， 添加映射端口
3. connect: nvme connect -t rdma -n "nqn.2022-06.h3c.com:onestorxxx_hostgroup__1000001" -a  172.87.29.65 -s 4420
4. disconnect: nvme disconnect -n nqn.2022-06.h3c.com:onestorxxx_hostgroup__1000001
4. 卷：ll /dev/disk/by-id/|grep nvme

服务端：
1.  /opt/h3c/bin/python3.9 /opt/h3c/script/onestord_blk/rpc.py  nvmf_get_subsystems [nqn]
2. /opt/h3c/bin/python3.9 /opt/h3c/script/onestord_blk/rpc.py  nvmf_subsystem_get_listeners nqn.2022-06.h3c.com:onestorxxx_hostgroup__1000001
3. /opt/h3c/bin/python3.9 /opt/h3c/script/onestord_blk/rpc.py  nvmf_subsystem_get_controllers nqn.2022-06.h3c.com:onestorxxx_hostgroup__1000001
4. /opt/h3c/bin/python3.9 /opt/h3c/script/onestord_blk/rpc.py  nvmf_subsystem_get_qpairs nqn.2022-06.h3c.com:onestorxxx_hostgroup__1000001
5. /opt/h3c/bin/python3.9 /opt/h3c/script/onestord_blk/rpc.py bdev_get_bdevs


1.测试subsystem关键配置信息展示，先创建主机/主机组，并完成卷映射，执行以下命令观察结果：
      1、展示主机/主机组信息：onestor tgtadm nvmf subsystems show [nqn]
      2、展示主机/主机组的监听信息：onestor tgtadm nvmf subsystems show-listeners {nqn}
      3、展示主机/主机组已开卷的信息：onestor tgtadm bdev show;
2.业务主机连接存储，执行以下命令观察结果：
      1、展示连接信息：onestor tgtadm nvmf subsystems show-controllers {nqn}
      2、展示连接的QP信息：onestor tgtadm nvmf subsystems show qpairs {nqn}; 

0705—-by HaoLiu
GLOBAL Resources:
target
	->> subsystem  <-> nqn 映射关系map_view
		->> namespace  -> 卷
		<-> add_listener 添加端口
		->> add_host 主机

tgt
transport 每个IO线程创建独立的网络
Add listener
add host
add ns 添加卷，每个IO线程bdev get channel


	
nvmf_poll_group <-> spdk_thread
controller -> session
qpair(sq, cq) -> connection


Ques:
Subsystem:
	主机，卷同时存在于多个subsystem， Discovery的时候会返回多个吗？可以登录吗？
	ana_group? 几种类型的SUBSYSTEM
	
	
0723--SPDK overview:
However, SPDK requires a number of operations that POSIX does not provide, such as enumerating the PCI devices on the system or allocating memory that is safe for DMA.
1. env.h
	- memory pool
	- cpu core, NUMA node, cpu affinity
	- spdk ring
	- pci device(VMD/IOAT/IDXD PCI driver object), pci bar, attach/detach, claim/unclaim, allow
	- cfg_read/write, pci event listen/error handler
	- address translate

2. app overview - THE LAYER
	iscsi target
	nvme over Fabrics target
	vhost target
	examples/**
	The framework defines a concept called a subsystem and all functionality is implemented in various subsystems.
	Subsystems have a unified initialization and teardown path.
	
 - vhost target:A vhost target provides a local storage service as a process running on a local machine.
   It is capable of exposing virtualized block devices to QEMU	instances or other arbitrary processes.
   可为QEMU-based VM 提供SPDK Vhost-SCSI device或者Vhost-blk device.
	
 - nvme over Fabrics target：
   The NVMe over Fabrics specification defines subsystems that can be exported over different transports. SPDK has chosen to call the software that exports these subsystems a "target", which is the term used for iSCSI.
   The Linux kernel also implements an NVMe-oF target and host, and SPDK is tested for interoperability with the Linux kernel implementations.
   
 - iSCSI Target
   SPDK bdevs
   iSCSI Target server/Target nodes
   portal groups
   initiator groups
   bdev/nvme, scsi/lun
  
3. lib - core of spdk
	Block Device: scsi_dev -> scsi_lun -> bdev -> bdev_hbd
		kernel bdev: scsi_dev -> scsi_lun -> bdev -> device driver 		
	The SPDK block device layer, often simply called bdev, is a C library intended to be equivalent to the operating system block storage layer that often sits immediately above the device drivers in a traditional kernel storage stack.


RPCs Stack:
scripts/rpc.py nvmf_create_transport -t RDMA -u 8192 -i 131072 -c 8192 (create_transport)

scripts/rpc.py bdev_malloc_create -b Malloc0 512 512
scripts/rpc.py nvmf_create_subsystem nqn.2016-06.io.spdk:cnode1 -a -s SPDK00000000000001 -d SPDK_Controller1（创建subsystem）
scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 Malloc0 (卷映射）
scripts/rpc.py nvmf_subsystem_add_listener nqn.2016-06.io.spdk:cnode1 -t rdma -a 192.168.100.8 -s 4420（对外端口访问）

Target有哪些资源：
 - subsystems
	- hosts, listeners, ns
	- ctrlrs
	- 同步到tgt的各个poll group中
 - rdma transports
	- rdma spec:
		- event_channel, pollfds, mempool(work request pool)
		- devices, ports
		- accept_poller(event fds[IB async, RDMA CM events])
		- poll_groups
	- general transport:*tgt, opts, listeners
	- IO logic???
 - bdev (g_malloc_disks)
 

0730 SPDK Destination:
Code level with examples:
	- memory pool
	- spdk ring(mutex free)
	- spdk thread
	- RDMA transport
	
Tested/Log level:
	- pci device
	- cpu core, NUMA node, cpu affinity
	- some examples

Docs level:
	- Nvme standard
	- vhost, virtio standard
	- Kernel, UserSpace Driver
	
Kernel(everyday cookie, not too deep, hand on it!!!):
 - videos, books
 - experiments, driver
 - ideas
GOT A PROJECT FINALLY.

TCP流量测试：
iperf -s -i 2 -l 4m -N
iperf -c 175.19.53.111 -i 1 -t 30 -N -l 4m

RDMA流量测试：
ib_write_bw -d mlx5_0 -s 4194304 -F -R --report_gbits
ib_write_bw -d mlx5_0 -i 1 175.19.53.111 -n 10000 -F -R -s 4194304 --report_gbits

ib_send_bw -d mlx5_0 -s 8192 -F -R --report_gbits
ib_send_bw -d mlx5_0 -i 1 175.19.53.111 -n 1000 -F -R -s 8192 --report_gbits


